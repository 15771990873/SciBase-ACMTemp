kis the most important parameter in a text categorization system based on thek-nearest neighbor algorithm (kNN). To classify a new document, thek-nearest documents in the training set are determined first. The prediction of categories for this document can then be made according to the category distribution among theknearest neighbors. Generally speaking, the class distribution in a training set is not even; some classes may have more samples than others. The system's performance is very sensitive to the choice of the parameterk. And it is very likely that a fixedkvalue will result in a bias for large categories, and will not make full use of the information in the training set. To deal with these problems, an improved kNN strategy, in which different numbers of nearest neighbors for different categories are used instead of a fixed number across all categories, is proposed in this article. More samples (nearest neighbors) will be used to decide whether a test document should be classified in a category that has more samples in the training set. The numbers of nearest neighbors selected for different categories are adaptive to their sample size in the training set. Experiments on two different datasets show that our methods are less sensitive to the parameterkthan the traditional ones, and can properly classify documents belonging to smaller classes with a largek. The strategy is especially applicable and promising for cases where estimating the parameterkvia cross-validation is not possible and the class distribution of a training set is skewed.