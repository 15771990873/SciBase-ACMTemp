Distance estimation of visually simulated self-motion is difficult, because one has to know or make assumptions about scene layout to judge ego speed. Discrimination of the travel distances of two sequentially simulated self-motions in the same scene can be performed quite accurately (Bremmer and Lappe 1999; Frenz et al., 2003). However, the indication of the perceived distance of a single movement in terms of a spatial interval results in a depth scaling error: Intervals are correlated with the true travel distance, but underestimate travel distance by about 25&percnt; (Frenz and Lappe, 2005). Here we investigated whether the inclusion of further depth cues (disparity/motion parallax/figural cues) in the virtual environment allows more veridical interval adjustment. Experiments were conducted on a large single projection screen and in a fully immersive computer-animated virtual environment (CAVE). Forward movements in simple virtual environments were simulated with distances between 1.5 and 13 m with varying speeds. Subjects indicated the perceived distance of each movement in terms of a depth interval on the virtual ground plane. We found good correlation between simulated and indicated distances, indicative of an internal representation of the perceived distance. The slopes of the fitted regression lines revealed an underestimation of distance by about 25&percnt; under all conditions. We conclude that estimation of travel distance from optic flow is subject to scaling when compared to static intervals in the environment, irrespective of additional depth cues.