Scaling beyond CMOS require a new combination of computing paradigm and new devices. In this context, memristor are often considered as best candidate to implement efficiently synapses in hardware neural networks. In this article, we analyze the impact of memristor parameter variability. We build an analytical model of the global reliability at the crossbar level. It is based on a supervised learning method with multilayer and redundancy extensions. Comparisons with Monte Carlo simulations of small neural network validate our analytical model. It can be used to extrapolate directly the reliability of large-scale neural system. Our extrapolations show that high defect rate and important parameter variability can be handle efficiency with a moderate amount of redundancy.