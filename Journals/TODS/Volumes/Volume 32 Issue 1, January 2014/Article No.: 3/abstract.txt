Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in information retrieval. However, accessing word-sequence statistics using inverted indexes requires unreasonable processing time or substantial space overhead. Establishing a balance between these competing space and time trade-offs can dramatically improve system performance.In this article, we present and analyze a new index structure designed to improve query efficiency in dependency retrieval models. By adapting a class of (ε, δ)-approximation algorithms originally proposed for sketch summarization in networking applications, we show how to accurately estimate statistics important in term-dependency models with low, probabilistically bounded error rates. The space requirements for the vocabulary of the index is only logarithmically linked to the size of the vocabulary.Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index ofn-grams consisting of between 1 and 4 words extracted from the GOV2 collection to less than 0.01&percnt; of the space requirements of the vocabulary of a full index. We also show that largern-gram queries can be processed considerably more efficiently than in current alternatives, such as positional and next-word indexes.