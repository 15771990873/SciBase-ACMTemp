This article describes a novel approach to localizing networks of embedded cameras and sensors. In this scheme, the cameras and the sensors are equipped with controllable light sources (either visible or infrared), which are used for signaling. Each camera node can then determine automatically the bearing to all of the nodes that are visible from its vantage point. By fusing these measurements with the measurements obtained from onboard accelerometers, the camera nodes are able to determine the relative positions and orientations of other nodes in the network.The method uses angular measurements derived from images, rather than range measurements derived from time-of-flight or signal attenuation. The scheme can be implemented relatively easily with commonly available components, and it scales well since the localization calculations exploit the sparse structure of the system of measurements. Additionally, the method provides estimates of camera orientation which cannot be determined solely from range measurements.The localization technology could serve as a basic capability on which higher-level applications could be built. The method could also be used to automatically survey the locations of sensors of interest, to implement distributed surveillance systems, or to analyze the structure of a scene, based on images obtained from multiple registered vantage points. It also provides a mechanism for integrating the imagery obtained from the cameras with the measurements obtained from distributed sensors.