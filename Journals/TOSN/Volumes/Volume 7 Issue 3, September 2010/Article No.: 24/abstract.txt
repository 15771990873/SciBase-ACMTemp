The efficiency of the time synchronization service in wireless sensor networks is tightly connected to the design of the radio, the quality of the clocking hardware, and the synchronization algorithm employed. While improvements can be made on all levels of the system, over the last few years most work has focused on the algorithmic level to minimize message exchange and in radio architectures to provide accurate time-stamping mechanisms. Surprisingly, the influences of the underlying clock system and its impact on the overall synchronization accuracy has largely been unstudied.In this work, we investigate the impact of the clocking subsystem on the time synchronization service and address, in particular, the influence of changes in environmental temperature on clock drift in highly duty-cycled wireless sensor nodes. We also develop formulas that help the system architect choose the optimal resynchronization period to achieve a given synchronization accuracy. We find that the synchronization accuracy has a two region behavior. In the first region, the synchronization accuracy is limited by quantization error, while int he second region changes in environmental temperature impact the achievable accuracy. We verify our analytic results in simulation and real hardware experiments.