Recommending new items for suitable users is an important yet challenging problem due to the lack of preference history for the new items. Noncollaborative user modeling techniques that rely on the item features can be used to recommend new items. However, they only use the past preferences of each user to provide recommendations for that user. They do not utilize information from the past preferences of other users, which can potentially be ignoring useful information. More recent factor models transfer knowledge across users using their preference information in order to provide more accurate recommendations. These methods learn a low-rank approximation for the preference matrix, which can lead to loss of information. Moreover, they might not be able to learn useful patterns given very sparse datasets. In this work, we present <scp>UFSM</scp>, a method for top-<i>n</i> recommendation of new items given binary user preferences. <scp>UFSM</scp> learns <b>U</b>ser-specific <b>F</b>eature-based item-<b>S</b>imilarity <b>M</b>odels, and its strength lies in combining two points: (1) exploiting preference information across all users to learn multiple global item similarity functions and (2) learning user-specific weights that determine the contribution of each global similarity function in generating recommendations for each user. <scp>UFSM</scp> can be considered as a sparse high-dimensional factor model where the previous preferences of each user are incorporated within his or her latent representation. This way, <scp>UFSM</scp> combines the merits of item similarity models that capture local relations among items and factor models that learn global preference patterns. A comprehensive set of experiments was conduced to compare <scp>UFSM</scp> against state-of-the-art collaborative factor models and noncollaborative user modeling techniques. Results show that <scp>UFSM</scp> outperforms other techniques in terms of recommendation quality. <scp>UFSM</scp> manages to yield better recommendations even with very sparse datasets. Results also show that <scp>UFSM</scp> can efficiently handle high-dimensional as well as low-dimensional item feature spaces.