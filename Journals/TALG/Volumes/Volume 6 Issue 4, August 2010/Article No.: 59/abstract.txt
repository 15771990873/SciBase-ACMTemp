We study a generalization of thek-median problem with respect to an arbitrary dissimilarity measure D. Given a finite setPof sizen, our goal is to find a setCof sizeksuch that the sum of errors D(P,C) &equals; âˆ‘p&in;Pminc&in;C{D(p,c)} is minimized. The main result in this article can be stated as follows: There exists a (1+&epsis;)-approximation algorithm for thek-median problem with respect to D, if the 1-median problem can be approximated within a factor of (1+&epsis;) by taking a random sample of constant size and solving the 1-median problem on the sample exactly. This algorithm requires timen2O(mklog(mk/&epsis;)), wheremis a constant that depends only on &epsis; and D. Using this characterization, we obtain the first linear time (1+&epsis;)-approximation algorithms for thek-median problem in an arbitrary metric space with bounded doubling dimension, for the Kullback-Leibler divergence (relative entropy), for the Itakura-Saito divergence, for Mahalanobis distances, and for some special cases of Bregman divergences. Moreover, we obtain previously known results for the Euclideank-median problem and the Euclideank-means problem in a simplified manner. Our results are based on a new analysis of an algorithm of Kumar et al. [2004].