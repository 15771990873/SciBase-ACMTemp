The Johnson-Lindenstrauss transform is a dimensionality reduction technique with a wide range of applications to theoretical computer science. It is specified by a distribution over projection matrices from Rn→ Rkwherek nand states thatk=O(&epsiv;−2log 1/δ) dimensions suffice to approximate the norm of any fixed vector in Rnto within a factor of 1 ±&epsiv;with probability at least 1 −δ. In this article, we show that this bound onkis optimal up to a constant factor, improving upon a previousΩ((&epsiv;−2log1/δ)/log(1/&epsiv;)) dimension bound of Alon. Our techniques are based on lower bounding the information cost of a novel one-way communication game and yield the first space lower bounds in a data stream model that depend on the error probabilityδ.For many streaming problems, the most naïve way of achieving error probabilityδis to first achieve constant probability, then take the median ofO(log 1/δ) independent repetitions. Our techniques show that for a wide range of problems, this is in fact optimal! As an example, we show that estimating the &ell;p-distance for anyp∈ [0,2] requiresΩ(&epsiv;−2lognlog 1/δ) space, even for vectors in {0,1}n. This is optimal in all parameters and closes a long line of work on this problem. We also show the number of distinct elements requiresΩ(&epsiv;−2log 1/δ+ logn) space, which is optimal if&epsiv;−2=Ω(logn). We also improve previous lower bounds for entropy in the strict turnstile and general turnstile models by a multiplicative factor ofΩ(log 1/δ). Finally, we give an application to one-way communication complexity under product distributions, showing that, unlike the case of constantδ, the VC-dimension does not characterize the complexity whenδ=o(1).