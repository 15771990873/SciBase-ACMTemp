Negotiator often rely on learning an opponent's behavior and on then using the knowledge gained to arrive at a better deal. However, in an electronic negotiation setting in which the parties involved are often unknown to (and therefore lack information about) each other, this learning has to be accomplished with only the bid offers submitted during an ongoing negotiation. In this article, we consider such a scenario and develop learning algorithms for electronic agents that use a common negotiation tactic, namely, the time-dependent tactic (TDT), in which the values of the negotiating issues are dependent on the time elapsed in the negotiation. Learning algorithms for this tactic have not been proposed in the literature. Our approach is based on using the derivatives of the Taylor's series approximation of the TDT function in a three-phase algorithm that enumerates over a partial discretized version of the solution space. Computational results with our algorithms are encouraging.