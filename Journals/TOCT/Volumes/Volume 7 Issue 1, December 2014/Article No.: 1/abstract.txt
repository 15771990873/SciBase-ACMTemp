We prove a lower bound on the amount of nonuniform advice needed by black-box reductions for the Dense Model Theorem of Green, Tao, and Ziegler, and of Reingold, Trevisan, Tulsiani, and Vadhan. The latter theorem roughly says that for every distributionDthat is δ-dense in a distribution that isε′-indistinguishable from uniform, there exists a “dense model” forD, that is, a distribution that isδ-dense in the uniform distribution and isε-indistinguishable fromD. Thisε-indistinguishability is with respect to an arbitrary small class of functionsF. For the natural case whereε′ ≥ Ω(εδ) andε≥δO(1), our lower bound implies that Ω(√(1/ε) log(1/δ)·log|F|) advice bits are necessary for a certain type of reduction that establishes a stronger form of the Dense Model Theorem (and which encompasses all known proofs of the Dense Model Theorem in the literature). There is only a polynomial gap between our lower bound and the best upper bound for this case (due to Zhang), which isO((1/ε2)log(1/δ)·log|F|). Our lower bound can be viewed as an analogue of list size lower bounds for list-decoding of error-correcting codes, but for “dense model decoding” instead.