Cache memories in embedded systems play an important role in reducing the execution time of applications. Various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions, improving the runtime over a purely hardware-managed cache. Novel embedded systems, such as Intelâ€™s XScale and ARM Cortex processors, facilitate locking one or more lines in cache; this feature is calledcache locking. We present a method in for instruction-cache locking that is able to reduce the average-case runtime of a program. We demonstrate that the optimal solution for instruction cache locking can be obtained in polynomial time. However, a fundamental lack of correlation between cache hardware and software program points renders such optimal solutions impractical.Instead, we propose two practical heuristics-based approaches to achieve cache locking. First, we present a static mechanism for locking the cache, in which the locked contents of the cache are kept fixed over the execution of the program. Next, we present a dynamic mechanism that accounts for changing program requirements at runtime. We devise a cost--benefit model to discover the memory addresses that should be locked in the cache. We implement our scheme inside a binary rewriter, widening the applicability of our scheme to binaries compiled using any compiler.Results obtained on a suite of MiBench benchmarks show that our static mechanism results in 20&percnt; improvement in the instruction-cache miss rate on average and up to 18&percnt; improvement in the execution time on average for applications having instruction accesses as a bottleneck, compared to no cache locking. The dynamic mechanism improves the cache miss rate by 35&percnt; on average and execution time by 32&percnt; on instruction-cache-constrained applications.