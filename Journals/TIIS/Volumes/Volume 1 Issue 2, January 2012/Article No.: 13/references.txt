Allopenna, P., Magnuson, J., and Tanenhaus, M. 1998. Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. J. Mem. Lang. 38, 4, 419--439.
Argyle, M. and Dean, J. 1965. Eye contact, distance and affiliation. Sociometry 28, 2, 89--304.
Argyle, M. and Ingham, J. 1972. Gaze, mutual gaze, and proximity. Semiotica 6, 32--49.
Baldwin, D. 1993. Early referential understanding: Infants' ability to recognize referential acts for what they are. Dev. Psych. 29, 5, 832--843.
Ballard, D., Hayhoe, M., Pook, P., and Rao, R. 1997. Deictic codes for the embodiment of cognition. Behav. Brain Sci. 20, 4, 723--742.
Nikolaus Bee , Johannes Wagner , Elisabeth André , Thurid Vogt , Fred Charles , David Pizzi , Marc Cavazza, Discovering eye gaze behavior during human-agent conversation in an interactive storytelling application, International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, November 08-10, 2010, Beijing, China[doi>10.1145/1891903.1891915]
Nikolaus Bee , Johannes Wagner , Elisabeth André , Fred Charles , David Pizzi , Marc Cavazza, Multimodal interaction with a virtual character in interactive storytelling, Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1, May 10-14, 2010, Toronto, Canada
Cynthia Breazeal , Brian Scassellati, Infant-like social interactions between a robot and a human caregiver, Adaptive Behavior, v.8 n.1, p.49-74, Jan. 2000[doi>10.1177/105971230000800104]
Brennan, S., Chen, X., Dickinson, C., Neider, M., and Zelinsky, G. 2008. Coordinating cognition: The costs and benefits of shared gaze during collaborative search. Cognition 106, 3, 1465--1477.
Timothy Brick , Matthias Scheutz, Incremental natural language processing for HRI, Proceedings of the ACM/IEEE international conference on Human-robot interaction, March 10-12, 2007, Arlington, Virginia, USA[doi>10.1145/1228716.1228752]
Butterworth, G. 200 I. Joint visual attention in infancy. In Blackwell Handbook of Infant Development 213--240.
Clark, H. and Krych, M. 2004. Speaking while monitoring addressees for understanding. J. Mem. Lang. 50, 1, 62--81.
Darren Gergle , Alan T. Clark, See what i'm saying?: using Dyadic Mobile Eye tracking to study collaborative reference, Proceedings of the ACM 2011 conference on Computer supported cooperative work, March 19-23, 2011, Hangzhou, China[doi>10.1145/1958824.1958892]
Griffin, Z. 2004. Why look&quest; Reasons for eye movements related to language production. In The Interface of Language, Vision, and Action: Eye Movements and the Visual World. 213--247.
Griffin, Z. and Bock, K. 2000. What the eyes say about speaking. Psych. Sci. 11, 4, 274.
Erdan Gu , Norman I. Badler, Visual attention and eye gaze during multiparty conversations with distractions, Proceedings of the 6th international conference on Intelligent Virtual Agents, August 21-23, 2006, Marina Del Rey, CA[doi>10.1007/11821830_16]
Hanna, J. and Brennan, S. 2007. Speakers' eye gaze disambiguates referring expressions early during face-to-face conversation. J. Mem. Lang. 57, 4, 596--615.
Hasegawa, D., Cassell, J., and Araki, K. 2010. The role of embodiment and perspective in direction-giving systems. In Proceedings of the AAAI Fall Workshop on Dialog with Robots.
Hayhoe, M. and Ballard, D. 2005. Eye movements in natural behavior. Trends Cogn. Sci. 9, 4, 188--194.
Hayhoe, M. M., Shrivastava, A., Mruczek, R., and Pelz, J. B. 2003. Visual memory and motor planning in a natural task. J. Vis. 3, 1, 49--63.
Michael Katzenmaier , Rainer Stiefelhagen , Tanja Schultz, Identifying the addressee in human-human-robot interactions based on head pose and speech, Proceedings of the 6th international conference on Multimodal interfaces, October 13-15, 2004, State College, PA, USA[doi>10.1145/1027933.1027959]
Michael Kipp , Patrick Gebhard, IGaze: Studying Reactive Gaze Behavior in Semi-immersive Human-Avatar Interactions, Proceedings of the 8th international conference on Intelligent Virtual Agents, September 01-03, 2008, Tokyo, Japan[doi>10.1007/978-3-540-85483-8_19]
Kramer, J. and Scheutz, M. 2007. ADE: A framework for robust complex robotic architectures. In Proceedings of the International Conference on Intelligent Robots and Systems.
Land, M., Mennie, N., and Rusted, J. 1999. The roles of vision and eye movements in the control of activities of daily living. Perception 28, 11, 1311--1328.
MacDorman, K. and Ishiguro, H. 2006. The uncanny advantage of using androids in cognitive and social science research. Interact. Stud. 7, 3, 297--337.
Meyer, A., Sleiderink, A., and Levelt, W. l998. Viewing and naming objects: Eye movements during noun phrase production. Cognition, 66, 2, 25--33.
Louis-Philippe Morency , C. Mario Christoudias , Trevor Darrell, Recognizing gaze aversion gestures in embodied conversational discourse, Proceedings of the 8th international conference on Multimodal interfaces, November 02-04, 2006, Banff, Alberta, Canada[doi>10.1145/1180995.1181051]
Mutlu, B., Forlizzi, J., and Hodgins, J. 2006. A storytelling robot: modeling and evaluation of human-like gaze behavior. In Proceedings of the 6th IEEE-RAS International Conference on Humanoid Robots. 518--523.
Yukiko I. Nakano , Gabe Reinstein , Tom Stocky , Justine Cassell, Towards a model of face-to-face grounding, Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, p.553-561, July 07-12, 2003, Sapporo, Japan[doi>10.3115/1075096.1075166]
Pelz, J., Hayhoe, M., and Loeber, R. 2001. The coordination of eye, head, and hand movements in a natural task. Exper. Brain Resear. 139, 3, 266--277.
Shaolin Qu , Joyce Y. Chai, Context-based word acquisition for situated dialogue in a virtual world, Journal of Artificial Intelligence Research, v.37 n.1, p.247-278, January 2010
Rayner, K. 1998. Eye movements in reading and information processing: 20 years of research. Psych. Bull. 124, 372--422.
Rehm, M. and Andre, E. 2005. Where do they look&quest; Gaze behaviors of multiple users interacting with an embodied conversational agent. In Proceedings of the 5th International Conference on Intelligent Virtual Agents.
Matthias Scheutz , Paul Schermerhorn , James Kramer , David Anderson, First steps toward natural human-like HRI, Autonomous Robots, v.22 n.4, p.411-423, May       2007[doi>10.1007/s10514-006-9018-3]
Shintel, H. and Keysar, B. 2009. Less is more: A minimalist account of joint action in communication. Topics Cogn. Sci. 1, 2, 260--273.
Shockley, K., Santana, M., and Fowler, C. 2003. Mutual interpersonal postural constraints are involved in cooperative conversation. J. Exper. Psych. 29, 2, 326--332.
Maria Staudte , Matthew W. Crocker, Visual attention in spoken human-robot interaction, Proceedings of the 4th ACM/IEEE international conference on Human robot interaction, March 09-13, 2009, La Jolla, California, USA[doi>10.1145/1514095.1514111]
Tanenhaus, M., Spivey-Knowlton, M., Eberhard, K., and Sedivy, J. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268, 5217, 1632--1634.
J. G. Trafton , N. L. Cassimatis , M. D. Bugajska , D. P. Brock , F. E. Mintz , A. C. Schultz, Enabling effective human-robot interaction using perspective-taking in robots, IEEE Transactions on Systems, Man, and Cybernetics, Part A: Systems and Humans, v.35 n.4, p.460-470, July 2005[doi>10.1109/TSMCA.2005.850592]
Roel Vertegaal, Introduction, Communications of the ACM, v.46 n.3, March 2003[doi>10.1145/636772.636794]
Akiko Yamazaki , Keiichi Yamazaki , Yoshinori Kuno , Matthew Burdelski , Michie Kawashima , Hideaki Kuzuoka, Precision timing in human-robot interaction: coordination of head movement and utterance, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, April 05-10, 2008, Florence, Italy[doi>10.1145/1357054.1357077]
Chen Yu , Dana H. Ballard, A multimodal learning interface for grounding spoken language in sensory perceptions, ACM Transactions on Applied Perception (TAP), v.1 n.1, p.57-80, July 2004[doi>10.1145/1008722.1008727]
Chen Yu , Matthias Scheutz , Paul Schermerhorn, Investigating multimodal real-time patterns of joint attention in an hri word learning task, Proceedings of the 5th ACM/IEEE international conference on Human-robot interaction, March 02-05, 2010, Osaka, Japan
Chen Yu , L. B. Smith , Hongwei Shen , A. F. Pereira , T. Smith, Active Information Selection: Visual Attention Through the Hands, IEEE Transactions on Autonomous Mental Development, v.1 n.2, p.141-151, August 2009[doi>10.1109/TAMD.2009.2031513]
Chen Yu , Thomas G. Smith , Shohei Hidaka , Matthias Scheutz , Linda B. Smith, A data-driven paradigm to understand multimodal communication in human-human and human-robot interaction, Proceedings of the 9th international conference on Advances in Intelligent Data Analysis, May 19-21, 2010, Tucson, AZ[doi>10.1007/978-3-642-13062-5_22]
Chen Yu , Yiwen Zhong , Thomas Smith , Ikhyun Park , Weixia Huang, Visual data mining of multimedia data for social and behavioral studies, Information Visualization, v.8 n.1, p.56-70, January 2009[doi>10.1057/ivs.2008.32]
Hui Zhang , Damian Fricker , Thomas G. Smith , Chen Yu, Real-time adaptive behaviors in multimodal human-avatar interactions, International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, November 08-10, 2010, Beijing, China[doi>10.1145/1891903.1891909]
