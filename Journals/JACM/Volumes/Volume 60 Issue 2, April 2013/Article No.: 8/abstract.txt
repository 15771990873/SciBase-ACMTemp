A common approach to clustering data is to view data objects as points in a metric space, and then to optimize a natural distance-based objective such as thek-median,k-means, or min-sum score. For applications such as clustering proteins by function or clustering images by subject, the implicit hope in taking this approach is that the optimal solution for the chosen objective will closely match the desired “target” clustering (e.g., a correct clustering of proteins by function or of images by who is in them). However, most distance-based objectives, including those mentioned here, are NP-hard to optimize. So, this assumption by itself is not sufficient, assuming P ≠ NP, to achieve clusterings of low-error via polynomial time algorithms.In this article, we show that we can bypass this barrier if we slightly extend this assumption to ask that for some small constantc, not only the optimal solution, but also allc-approximations to the optimal solution, differ from the target on at most some &epsi; fraction of points—we call this(c,&epsi;)-approximation-stability. We show that under this condition, it is possible to efficiently obtain low-error clusterings even if the property holds only for valuescfor which the objective is known to be NP-hard to approximate. Specifically, for any constantc > 1, (c,&epsi;)-approximation-stability ofk-median ork-means objectives can be used to efficiently produce a clustering of errorO(&epsi;) with respect to the target clustering, as can stability of the min-sum objective if the target clusters are sufficiently large. Thus, we can perform nearly as well in terms of agreement with the target clusteringas ifwe could approximate these objectives to this NP-hard value.