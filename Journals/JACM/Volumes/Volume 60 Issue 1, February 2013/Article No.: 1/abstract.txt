Ye [2011] showed recently that the simplex method with Dantzig’s pivoting rule, as well as Howard’spolicy iterationalgorithm, solve discounted Markov decision processes (MDPs), with a constant discount factor, in strongly polynomial time. More precisely, Ye showed that both algorithms terminate after at mostO(mn1−γlogn1−γ) iterations, wherenis the number of states,mis the total number of actions in the MDP, and 0 <γ< 1 is the discount factor. We improve Ye’s analysis in two respects. First, we improve the bound given by Ye and show that Howard’s policy iteration algorithm actually terminates after at mostO(m1−γlogn1−γ) iterations. Second, and more importantly, we show that the same bound applies to the number of iterations performed by thestrategy iteration(orstrategy improvement) algorithm, a generalization of Howard’s policy iteration algorithm used for solving 2-player turn-basedstochastic gameswith discounted zero-sum rewards. This provides the first strongly polynomial algorithm for solving these games, solving a long standing open problem. Combined with other recent results, this provides a complete characterization of the complexity the standard strategy iteration algorithm for 2-player turn-based stochastic games; it is strongly polynomial for a fixed discount factor, and exponential otherwise.