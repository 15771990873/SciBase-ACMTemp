Upper bounds for the error probability of a Bayes decision function are derived in terms of the differences among the probability distributions of the features used in character recognition. Applications to feature selection and error reduction are discussed. It is shown that if a sufficient number of well-selected features is used, the error probability can be made arbitrarily small.