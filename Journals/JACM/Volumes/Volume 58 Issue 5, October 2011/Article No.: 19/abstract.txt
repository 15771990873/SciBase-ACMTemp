Thek-means method is one of the most widely used clustering algorithms, drawing its popularity from its speed in practice. Recently, however, it was shown to have exponential worst-case running time. In order to close the gap between practical performance and theoretical analysis, thek-means method has been studied in the model of smoothed analysis. But even the smoothed analyses so far are unsatisfactory as the bounds are still super-polynomial in the numbernof data points.In this article, we settle the smoothed running time of thek-means method. We show that the smoothed number of iterations is bounded by a polynomial innand 1/σ, whereσis the standard deviation of the Gaussian perturbations. This means that if an arbitrary input data set is randomly perturbed, then thek-means method will run in expected polynomial time on that input set.